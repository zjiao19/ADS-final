{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import linear_kernel # for cosine similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>platform</th>\n",
       "      <th>release_date</th>\n",
       "      <th>summary</th>\n",
       "      <th>meta_score</th>\n",
       "      <th>user_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Legend of Zelda: Ocarina of Time</td>\n",
       "      <td>Nintendo 64</td>\n",
       "      <td>November 23, 1998</td>\n",
       "      <td>As a young boy, Link is tricked by Ganondorf, ...</td>\n",
       "      <td>99</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tony Hawk's Pro Skater 2</td>\n",
       "      <td>PlayStation</td>\n",
       "      <td>September 20, 2000</td>\n",
       "      <td>As most major publishers' development efforts ...</td>\n",
       "      <td>98</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Theft Auto IV</td>\n",
       "      <td>PlayStation 3</td>\n",
       "      <td>April 29, 2008</td>\n",
       "      <td>[Metacritic's 2008 PS3 Game of the Year; Also ...</td>\n",
       "      <td>98</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SoulCalibur</td>\n",
       "      <td>Dreamcast</td>\n",
       "      <td>September 8, 1999</td>\n",
       "      <td>This is a tale of souls and swords, transcendi...</td>\n",
       "      <td>98</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Super Mario Galaxy</td>\n",
       "      <td>Wii</td>\n",
       "      <td>November 12, 2007</td>\n",
       "      <td>[Metacritic's 2007 Wii Game of the Year] The u...</td>\n",
       "      <td>97</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12249</th>\n",
       "      <td>Charlie's Angels</td>\n",
       "      <td>GameCube</td>\n",
       "      <td>July 9, 2003</td>\n",
       "      <td>Join Natalie, Dylan, and Alex for an intense a...</td>\n",
       "      <td>23</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12250</th>\n",
       "      <td>Fast &amp; Furious: Showdown</td>\n",
       "      <td>Xbox 360</td>\n",
       "      <td>May 21, 2013</td>\n",
       "      <td>Fast &amp; Furious: Showdown takes some of the fra...</td>\n",
       "      <td>22</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12251</th>\n",
       "      <td>Drake of the 99 Dragons</td>\n",
       "      <td>Xbox</td>\n",
       "      <td>November 3, 2003</td>\n",
       "      <td>Drake is out for revenge in a supernatural Hon...</td>\n",
       "      <td>22</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12252</th>\n",
       "      <td>Afro Samurai 2: Revenge of Kuma Volume One</td>\n",
       "      <td>PlayStation 4</td>\n",
       "      <td>September 22, 2015</td>\n",
       "      <td>Head out on a journey of redemption, driven by...</td>\n",
       "      <td>21</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12253</th>\n",
       "      <td>Infestation: Survivor Stories (The War Z)</td>\n",
       "      <td>PC</td>\n",
       "      <td>October 15, 2012</td>\n",
       "      <td>(Formerly known as \"The War Z\") It has been 5 ...</td>\n",
       "      <td>20</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12153 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             name        platform  \\\n",
       "0            The Legend of Zelda: Ocarina of Time     Nintendo 64   \n",
       "1                        Tony Hawk's Pro Skater 2     PlayStation   \n",
       "2                             Grand Theft Auto IV   PlayStation 3   \n",
       "3                                     SoulCalibur       Dreamcast   \n",
       "4                              Super Mario Galaxy             Wii   \n",
       "...                                           ...             ...   \n",
       "12249                            Charlie's Angels        GameCube   \n",
       "12250                    Fast & Furious: Showdown        Xbox 360   \n",
       "12251                     Drake of the 99 Dragons            Xbox   \n",
       "12252  Afro Samurai 2: Revenge of Kuma Volume One   PlayStation 4   \n",
       "12253   Infestation: Survivor Stories (The War Z)              PC   \n",
       "\n",
       "             release_date                                            summary  \\\n",
       "0       November 23, 1998  As a young boy, Link is tricked by Ganondorf, ...   \n",
       "1      September 20, 2000  As most major publishers' development efforts ...   \n",
       "2          April 29, 2008  [Metacritic's 2008 PS3 Game of the Year; Also ...   \n",
       "3       September 8, 1999  This is a tale of souls and swords, transcendi...   \n",
       "4       November 12, 2007  [Metacritic's 2007 Wii Game of the Year] The u...   \n",
       "...                   ...                                                ...   \n",
       "12249        July 9, 2003  Join Natalie, Dylan, and Alex for an intense a...   \n",
       "12250        May 21, 2013  Fast & Furious: Showdown takes some of the fra...   \n",
       "12251    November 3, 2003  Drake is out for revenge in a supernatural Hon...   \n",
       "12252  September 22, 2015  Head out on a journey of redemption, driven by...   \n",
       "12253    October 15, 2012  (Formerly known as \"The War Z\") It has been 5 ...   \n",
       "\n",
       "       meta_score user_review  \n",
       "0              99         9.1  \n",
       "1              98         7.4  \n",
       "2              98         7.7  \n",
       "3              98         8.4  \n",
       "4              97         9.1  \n",
       "...           ...         ...  \n",
       "12249          23         4.3  \n",
       "12250          22         1.3  \n",
       "12251          22         1.7  \n",
       "12252          21         2.9  \n",
       "12253          20         1.7  \n",
       "\n",
       "[12153 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('all_games.csv')\n",
    "data.drop_duplicates('name',keep='first', inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.drop(index=data[data['summary'].isnull()].index, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/yuzhengzhang/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     df\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mdummy\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n\u001b[0;32m---> 63\u001b[0m proc_data \u001b[39m=\u001b[39m nlp(data, \u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [14], line 14\u001b[0m, in \u001b[0;36mnlp\u001b[0;34m(df, text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Word & Sentence Tokenization\u001b[39;00m\n\u001b[1;32m     13\u001b[0m token_post \u001b[39m=\u001b[39m (word_tokenize(post) \u001b[39mfor\u001b[39;00m post \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mdummy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m token_post \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m token_post]\n\u001b[1;32m     17\u001b[0m \u001b[39m# Remove Punctuation\u001b[39;00m\n\u001b[1;32m     18\u001b[0m reg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39m(@[a-z0-9]+)|([^0-9a-z \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m])|(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [14], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Word & Sentence Tokenization\u001b[39;00m\n\u001b[1;32m     13\u001b[0m token_post \u001b[39m=\u001b[39m (word_tokenize(post) \u001b[39mfor\u001b[39;00m post \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mdummy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m token_post \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m token_post]\n\u001b[1;32m     17\u001b[0m \u001b[39m# Remove Punctuation\u001b[39;00m\n\u001b[1;32m     18\u001b[0m reg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39m(@[a-z0-9]+)|([^0-9a-z \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m])|(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [14], line 13\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mdummy\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dummy]\n\u001b[1;32m     12\u001b[0m \u001b[39m# Word & Sentence Tokenization\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m token_post \u001b[39m=\u001b[39m (word_tokenize(post) \u001b[39mfor\u001b[39;00m post \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mdummy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m token_post \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m token_post]\n\u001b[1;32m     17\u001b[0m \u001b[39m# Remove Punctuation\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/yuzhengzhang/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# clean/process corpus\n",
    "\n",
    "# Function to streamline NLP Process\n",
    "\n",
    "def nlp(df, text):\n",
    "    \n",
    "    df['dummy'] = df[text].astype(str)\n",
    "    # Convert to lowercase\n",
    "    dummy = (post.lower() for post in df['dummy'])\n",
    "    df['dummy'] = [i for i in dummy]\n",
    "\n",
    "    # Word & Sentence Tokenization\n",
    "    token_post = (word_tokenize(post) for post in df['dummy'])\n",
    "    token_post = [i for i in token_post]\n",
    "\n",
    "    \n",
    "    # Remove Punctuation\n",
    "    reg = re.compile('(@[a-z0-9]+)|([^0-9a-z \\t])|(\\w+:\\/\\/\\S+)')\n",
    "\n",
    "    no_punc = []\n",
    "\n",
    "    for filt in token_post:\n",
    "        review = []\n",
    "        for token in filt:\n",
    "            new_token = reg.sub(u'', token)\n",
    "            if not new_token == u'':\n",
    "                review.append(new_token)\n",
    "        no_punc.append(review)\n",
    "        \n",
    "    # Remove Stopwords\n",
    "    no_stop = []\n",
    "\n",
    "    for post in no_punc:\n",
    "        new_term_vector = []\n",
    "        for word in post:\n",
    "            if not word in stopwords.words('english'):\n",
    "                new_term_vector.append(word)\n",
    "\n",
    "        no_stop.append(new_term_vector)\n",
    "        \n",
    "    # Stemming & Lemmatization\n",
    "    pstem = PorterStemmer()\n",
    "    wlem = WordNetLemmatizer()\n",
    "\n",
    "    preproc_text = []\n",
    "\n",
    "    for text in no_stop:\n",
    "        final_text = []\n",
    "        for word in text:\n",
    "            pstem.stem(word)\n",
    "            final_text.append(wlem.lemmatize(word))\n",
    "\n",
    "        preproc_text.append(final_text)\n",
    "        \n",
    "    # create final data set\n",
    "    #data = df.copy()\n",
    "\n",
    "    new_col = pd.Series(preproc_text)\n",
    "    df['proc_summary'] = new_col\n",
    "    df.drop('dummy', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "proc_data = nlp(data, 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
